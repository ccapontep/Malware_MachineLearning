# -*- coding: utf-8 -*-
"""
Created on Wed Oct 31 15:30:50 2018

@author: Cecilia Aponte
Machine Learning
Homework 1
"""

###Importing the libraries
import os, codecs
import numpy as np
import pandas as pd
from collections import Counter
import pickle

# Libraries to clean text
import re

directory = os.getcwd()



######## START: Selection of families that have more than 20 samples ##########

###Importing the family data info
dataset = pd.read_csv('sha256_family.csv')
fam_data = dataset.iloc[:, 1]. values
name_data = dataset.iloc[:, 0]. values


### Locate the families that have more than 20 examples
fam_add = []
fam_del = []
count_fam = Counter(fam_data)
for key, value in count_fam.items():
    if value >= 20:
        fam_add.append(key) # list of names of all families to be add
        
        
index_add = []
fam_list = []
name_list = []

### Find index of families with more than 20 examples
for index, family in enumerate(fam_data):
    for fam_name in fam_add:
        if family == fam_name:
            index_add.append(index) # append index number to list


### Remove from fam_list and name_list all instances that is less than 20 
for index in index_add:
    fam_list.append(fam_data[index])
    name_list.append(name_data[index])


# Note: deleted 175 families that contained less than 20 instances. This
# equates to 775 instances, so from 5560 ended up with 4785 instances total.



######### END: Selection of families that have more than 20 samples ###########
    
    

###############################################################################
    
    

###### START: Feature Extraction - Set samples as With or Without Virus #######


### Load all samples
SAMPLES_DIR = "feature_vectors"
temp_pos = []
pos_virus = []


### If list of positive virus features is already created, load. Otherwise 
#   run for the first time by running the script in this section
if 'pos_virus_list' in  os.listdir(directory):
    pos_virus = pickle.load(open('pos_virus_list', 'rb'))
    print("Done loading saved positive virus feature file.")
else:
    ## Create a list of a list of vectors for each POSITIVE instance of a virus
#    for name in name_list:
    for file_n in os.listdir(SAMPLES_DIR):
        if file_n in name_list: # if it is a virus
            with codecs.open(os.path.join(SAMPLES_DIR, file_n), "r") as file: 
                for line in file.readlines(): 
                    # Remplaces :: tp !! characters after call of main element 
                    # To make sure there aren't more :: that get split in the 
                    # main info feature call 
                    line = re.sub('::', "!!", line) 
                    split = line.split('!!')
                    # add the feature in a list with [name, feature]
                    temp_pos += [split] 
            pos_virus += [temp_pos]
            temp_pos = []
    print("Done with adding positive virus samples!")
    pickle.dump(pos_virus, open("pos_virus_list", 'wb'))
    print("Done with saving file of positive virus samples!")

neg_virus = []
temp_neg = []

### If list of positive virus features is already created, load. Otherwise 
#   run for the first time by running the script in this section
if 'neg_virus_list' in  os.listdir(directory):
    neg_virus = pickle.load(open('neg_virus_list', 'rb'))
    print("Done loading saved negative virus feature file.")
else:
    ## Create a list of a list of vectors for each NEGATIVE instance of a virus
    count_index = 0
    for file_n in os.listdir(SAMPLES_DIR):
        if file_n not in name_data: # if it's not a virus
            with codecs.open(os.path.join(SAMPLES_DIR, file_n), "r") as file: 
                for line in file.readlines(): 
                    # Remplaces :: tp !! characters after call of main element 
                    # To make sure there aren't more :: that get split in the 
                    # main info feature call                      
                    line = re.sub('::', "!!", line) 
                    split = line.split('!!')
                    # add the feature in a list with [name, feature]
                    temp_neg += [split]
            neg_virus += [temp_neg]
            temp_neg = []
            count_index += 1
#            print(count_index)
    print("Done with adding negative virus samples!")
    pickle.dump(neg_virus, open("neg_virus_list", 'wb'))
    print("Done with saving file of negative virus samples!")
                

###### END: Feature Extraction - Set samples as With or Without Virus #######



##############################################################################
    


########################### START: Clean data  ###############################

import random

### Add output of sample as either True for virus detection "1", or False "0"
for sample in neg_virus:
    sample.append("0")

for sample in pos_virus:    
    sample.append("1")

### Add all data into one list, then shuffle so it is ready to be split into 
#   train and test sets
data_clean = neg_virus + pos_virus
random.shuffle(data_clean)

# Save data as a file
pickle.dump(data_clean, open("data_clean", 'wb'))
    
# Open data_clean file
if 'data_clean' in  os.listdir(directory):
    data_clean = pickle.load(open('data_clean', 'rb'))


### Test to see how many distinct items are in each feature set to see how to 
#   construct the one-hot encoder
service_receiver_list = []

for sample in data_clean:
    for item in sample:
        if item[0] == "service_receiver":
            service_receiver_list.append(item[1])
service_receiver_count = []
count_feat = Counter(service_receiver_list)
for key, value in count_feat.items():
    if value >= 2:
        service_receiver_count.append(key) 
        
activity_list = []

for sample in data_clean:
    for item in sample:
        if item[0] == "activity":
            activity_list.append(item[1])
activity_count = []
count_feat = Counter(activity_list)
for key, value in count_feat.items():
    if value >= 2:
        activity_count.append(key) 
        
provider_list = []

for sample in data_clean:
    for item in sample:
        if item[0] == "provider":
            provider_list.append(item[1])
provider_count = []
count_feat = Counter(provider_list)
for key, value in count_feat.items():
    if value >= 2:
        provider_count.append(key) 

intent_list = []

for sample in data_clean:
    for item in sample:
        if item[0] == "intent":
            intent_list.append(item[1])
intent_count = []
count_feat = Counter(intent_list)
for key, value in count_feat.items():
    if value >= 2:
        intent_count.append(key) 

permission_list = []

for sample in data_clean:
    for item in sample:
        if item[0] == "permission":
            permission_list.append(item[1])
permission_count = []
count_feat = Counter(permission_list)
for key, value in count_feat.items():
    if value >= 2:
        permission_count.append(key) 
  
permission_list = list(set(permission_list))      


#    "feature": 71
#    "permission": 3,798
#    "activity": 185,215 ** SIZE TOO BIG FOR ONE-HOT - won't include, too sparse
#    "service_receiver": 32,714  ** MEMORY ERROR AT ONE-HOT, HAVE TO REMOVE
#    "provider": 4,500
#    "service": 0 ********* REMOVE, NOT NEEDED SINCE NONE
#    "intent": 6,374
#    "api_call": 315
#    "real_permission": 70
#    "call": 732
#    "url": 308,962 ** SIZE TOO BIG FOR ONE-HOT - won't include, too sparse


### Remove "url" features from dataset 
### Remove columns that only have one instance, so to remove sparcity since one
#   instance of something is not enough to make a significant trend. Also 
#   because of memoryerror problem, since the array is too big to be handled
    
data_updated = []
sample_update = []

for index_s, sample in enumerate(data_clean):
    for index, item in enumerate(sample):
        if item[0] == "activity" and item[1] in activity_count:
            sample_update += [item]
        elif item[0] == "service_receiver" and item[1] in service_receiver_count:
            sample_update += [item]
        elif item[0] == "provider" and item[1] in provider_count:
            sample_update += [item] 
        elif item[0] == "intent" and item[1] in intent_count:
            sample_update += [item]
        elif item[0] == "permission" and item[1] in permission_count:
            sample_update += [item]
        elif item[0] == 'feature' or item[0] == 'api_call' or item[0] == 'real_permission' or item[0] == 'call':
            sample_update += [item]
        elif item[0] == '0' or item[0] == '1':
            sample_update += [item] 
    data_updated += [sample_update]
    sample_update = []
    
#    "feature": 71
#    "permission": 3,798            -> 1,095
#    "activity": 185,215            -> 56,048
#    "service_receiver": 32,714     -> 11,492
#    "provider": 4,500              -> 1,250
#    "intent": 6,374                -> 2,497
#    "api_call": 315
#    "real_permission": 70
#    "call": 732
   

# Save data as a file
pickle.dump(data_updated, open("data_updated", 'wb'))
    
# Open data_updated file
if 'data_updated' in  os.listdir(directory):
    data_updated = pickle.load(open('data_updated', 'rb'))


########################### END: Clean data  ##################################



###############################################################################




################### START: Create input/output (x,y) lists  ###################


### Split the data into input X and output y
X = []
y = []

for sample in data_updated:
    for index, item in enumerate(sample):
        if index == len(sample) - 1 :
            y.append(item) # append the last index which is the y_output

for sample in data_updated:
    for index, item in enumerate(sample):
        if index == len(sample) - 1 :
            sample.pop(index) # remove the last index which is the y_output
            X.append(sample)

# Save X and y as files
pickle.dump(X, open("X", 'wb'))
pickle.dump(y, open("y", 'wb'))

# Open saved files
X = pickle.load(open('X', 'rb'))
y = pickle.load(open('y', 'rb'))


##################### END: Create input/output (x,y) lists ####################



##############################################################################



############# START: One-hot encoding of vector features ####################
    
# Feature dictionary
FEATURES_DIC = {
    "feature": 1,
    "permission": 2,
    "provider": 3,
    "intent": 4,
    "api_call": 5,
    "real_permission": 6,
    "call": 7, 
    "service_receiver": 8,
    "activity": 9}

# Replace the column names by checking the dictionary & setting a number to it
for index_s, sample in enumerate(X):
    for index_i, item in enumerate(sample):
        item[0] = FEATURES_DIC.get(item[0])


### Reshape lists of lists into a dataframe by adding columns for each feature
col_1 = []; col_2 = []; col_3 = []; col_4 = []; col_5 = []; col_6 = []; col_7 = []; 
col_8 = []; col_9 = [];
Col_1 = []; Col_2 = []; Col_3 = []; Col_4 = []; Col_5 = []; Col_6 = []; Col_7 = []
Col_8 = []; Col_9 = [];



# Get each feature as a separate column
for index_s, sample in enumerate(X):
    for index, item in enumerate(sample):
        if item[0] == 1:
            col_1 += [item[1]] 
        elif item[0] == 2:
            col_2 += [item[1]]
        elif item[0] == 3:
            col_3 += [item[1]]
        elif item[0] == 4:
            col_4 += [item[1]]
        elif item[0] == 5:
            col_5 += [item[1]]
        elif item[0] == 6:
            col_6 += [item[1]]
        elif item[0] == 7:
            col_7 += [item[1]]
        elif item[0] == 8:
            col_8 += [item[1]]
        elif item[0] == 9:
            col_9 += [item[1]]
    Col_1 += [col_1]; Col_2 += [col_2]; Col_3 += [col_3]; Col_4 += [col_4]
    Col_5 += [col_5]; Col_6 += [col_6]; Col_7 += [col_7]; Col_8 += [col_8]
    Col_9 += [col_9]; 
    col_1 = []; col_2 = []; col_3 = []; col_4 = []; col_5 = []; col_6 = []; col_7 = []
    col_8 = []; col_9 = []; 



# Column 8 and 9 are too big, memory overload. So will have to divide into small sizes 
# and then concatinate them so they contain 4,000 features at most
#
#col_8_1 = []; col_8_2 = []; col_8_3 = []
#Col_8_1 = []; Col_8_2 = []; Col_8_3 = []
#
#for index_s, sample in enumerate(Col_8):
#    for index, item in enumerate(sample):
#        # Divide the features that have too many into smaller sizes, so that later
#        # can create data that is smaller and doesn't case memory errors
#        if item in service_receiver_count[0:4000]:
#            col_8_1 += [item]
#        elif item in service_receiver_count[4000:8000]:
#            col_8_2 += [item]
#        elif item in service_receiver_count[8000:]:
#            col_8_3 += [item]
#    Col_8_1 += [col_8_1]; Col_8_2 += [col_8_2]; Col_8_3 += [col_8_3]
#    col_8_1 = []; col_8_2 = []; col_8_3 = []
   
    
    
# Save file
pickle.dump(Col_1, open("Col_1", 'wb')); pickle.dump(Col_2, open("Col_2", 'wb'))
pickle.dump(Col_3, open("Col_3", 'wb')); pickle.dump(Col_4, open("Col_4", 'wb'))
pickle.dump(Col_5, open("Col_5", 'wb')); pickle.dump(Col_6, open("Col_6", 'wb'))
pickle.dump(Col_7, open("Col_7", 'wb')); pickle.dump(Col_8, open("Col_8", 'wb'))
pickle.dump(Col_9, open("Col_9", 'wb'))

    
# Open saved files
Col_1 = pickle.load(open("Col_1", 'rb')); Col_2 = pickle.load(open("Col_2", 'rb'))
Col_3 = pickle.load(open("Col_3", 'rb')); Col_4 = pickle.load(open("Col_4", 'rb'))
Col_5 = pickle.load(open("Col_5", 'rb')); Col_6 = pickle.load(open("Col_6", 'rb'))
Col_7 = pickle.load(open("Col_7", 'rb')); Col_8 = pickle.load(open("Col_8", 'rb'))
Col_9 = pickle.load(open("Col_9", 'rb'))



###############################################################################


### Create one-hot vector from the categories. Since there is more than one
# variable per each sample, have to seperate each option into a separate column
# another method shown below, but it takes longer by getting the dummy variables
   
from sklearn.preprocessing import MultiLabelBinarizer   
from scipy.sparse import save_npz, load_npz
from scipy.sparse import hstack

# Binearize the data since samples contain more than one label
# Slower method to transform the multiple items per column to more columns
#S1_clean = X_matrix['S1'].str.get_dummies(sep = '||')

mlb = MultiLabelBinarizer(sparse_output=True)

# Transform to a binary array and remove extra dummy variable (dummy var trap)
# since the last dummy variable can be predicted by the other N-1 dummies
# Otherwise, this introduces a heavy collinearity between your dummy variables 
# (which is a very undesirable thing in linear/logistic regression) 
# order C is used for contiguous array, so it will use less memory

# Remove the dummy variable  -- another way to remove, shown below
#S1_clean = hstack([S1_clean[:, :2], S1_clean[:, 2:]]) # Remove column 2

S1_clean = mlb.fit_transform(Col_1); S1_clean = S1_clean[:, 1:]
S2_clean = mlb.fit_transform(Col_2); S2_clean = S2_clean[:, 1:]
S3_clean = mlb.fit_transform(Col_3); S3_clean = S3_clean[:, 1:]
S4_clean = mlb.fit_transform(Col_4); S4_clean = S4_clean[:, 1:]
S5_clean = mlb.fit_transform(Col_5); S5_clean = S5_clean[:, 1:]
S6_clean = mlb.fit_transform(Col_6); S6_clean = S6_clean[:, 1:]
S7_clean = mlb.fit_transform(Col_7); S7_clean = S7_clean[:, 1:]
S8_clean = mlb.fit_transform(Col_8); S8_clean = S8_clean[:, 1:]
S9_clean = mlb.fit_transform(Col_9); S9_clean = S9_clean[:, 1:]

# Check that the shape is correct
S1_clean_shape = S9_clean.get_shape()

#    "feature": 70
#    "permission": 3,798            -> 1,094
#    "activity": 185,215            -> 56,047
#    "service_receiver": 32,714     -> 11,491
#    "provider": 4,500              -> 1,249
#    "intent": 6,374                -> 2,496
#    "api_call": 314
#    "real_permission": 69
#    "call": 731
#  ___________________________________________
#   "total":                        73,561 features

S19_clean = hstack([S1_clean, S2_clean, S3_clean, S4_clean, S5_clean, S6_clean, 
                    S7_clean, S8_clean, S9_clean], format = 'csr')

S19_clean_shape = S19_clean.get_shape()


# Save / Load
save_npz('S19_clean', S19_clean)
S19_clean = load_npz('S19_clean.npz')

############### END: One-hot encoding of vector features ######################



###############################################################################



############## START: Splitting data between train and test ###################


# Split INDEX NUMBER at the start and end of data
num_start = int(S19_clean_shape[0] * 0.75)
num_end = int(S19_clean_shape[0] * 0.25) + 1 # add 1 since integer rounds down
add = num_start + num_end # add values to check it is same size as original

# Create X_train and X_test by splitting with 0.75 of data
X_train = S19_clean[0:num_start, :]
X_test = S19_clean[num_start:add, :]

# Get shapes
X_train_shape = X_train.get_shape()
X_test_shape = X_test.get_shape()


# Open y files 
y = pickle.load(open('y', 'rb'))

# Create y_train and test by splitting with 0.75 of data
y_train = np.array(y[0:num_start])
y_test = np.array(y[num_start:add])


# Save / Load     --- X data
save_npz('X_train', X_train)
save_npz('X_test', X_test)

X_train = load_npz(X_train)
X_test = load_npz(X_test)


# Save / Load      --- y data
pickle.dump(y_train, open("y_train", 'wb'))
pickle.dump(y_test, open("y_test", 'wb'))

y_train = pickle.load(open('y_train', 'rb'))
y_test = pickle.load(open('y_test', 'rb'))



############### END: Splitting data between train and test ####################



###############################################################################



### START: Fitting Classifier to the Training & Predicting the Test results ###


### Load data
X_train = load_npz('X_train.npz')
X_test = load_npz('X_test.npz')
y_train = pickle.load(open('y_train', 'rb'))
y_test = pickle.load(open('y_test', 'rb'))


### Fitting Classifier to the Training set

########### LINEAR

# Linear SVC
from sklearn.svm import LinearSVC 
classifier = LinearSVC(penalty='l2', loss='hinge', dual=True, 
                       tol=0.0001, C=1, fit_intercept=True, 
                       intercept_scaling=1, class_weight=None, max_iter=1000, 
                       random_state = 6) 

# Random Forest
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 100, criterion = 'gini', 
                                    verbose = 2, max_features = 5000,
                                    random_state = 6)


########## NON-LINEAR

# K-Nearest Neighbors    ----->>> MMEMORY ERROR
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, weights = 'distance', 
                                  metric = 'minkowski', p = 2)

# Non-linear SVM
from sklearn.svm import SVC 
classifier = SVC(C=1, kernel='rbf', degree=3, gamma=0.1, 
                 coef0=0.0, shrinking=True, probability=False, tol=0.001, 
                 class_weight=None, verbose=2, max_iter=100, 
                 decision_function_shape='ovr', random_state=6)

# Fit the training data to the classifier
classifier.fit(X_train, y_train) 

### Predicting the Test set results
y_pred = classifier.predict(X_test)     


### Making the Confusion Matrix
# matrix will contain the correct and incorrect predictions made by the model
# NOT THE BEST WAY TO EVALUATE THE MODEL SO K-FOLD WILL DO IT BETTER
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

cm = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
classification_Report = classification_report(y_test, y_pred, target_names 
                                              = ['Neg. Virus', 'Pos. Virus'], 
                                              digits =3)
print(classification_Report)
print('Accuracy is: ', accuracy)
print('Confusion Matrix is: ')
print(cm)

### END: Fitting Classifier to the Training & Predicting the Test results ###


##############################################################################


################### START: K-fold validation on results #######################

### Applying k-Fold Cross Validation
from sklearn.model_selection import cross_val_score

# return 10 accuracy of each fold combination
accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, 
                             cv = 10)
print('Mean of k-fold accuracies is: ', accuracies.mean()) 
# mean of all the 10 values and got 0.9959
print('Standard Deviation of k-fold accuracies is: ', accuracies.std()) 
# 0.000695 (0.07%), not very high variance meaning we are in low bias, 
# low variance category



################### END: K-fold validation on results #######################


###############################################################################


## START: Applying Grid Search to find the best model and the best parameters ##


from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score


# Parameters is a list of dictionaries of the parameters that need to be tested 
# to find the most optimal value. So will be inputting different values for each
# then GridSearchCV will find the most optimal one.
# to check names for parameters: get_params().keys()

# C is the penalty parameter, don't get it too high otherwise it will underfit


### Linear:

# For Linear SVC -- Full Grid Search
parameters = [ {'C': [0.1, 1, 10, 100, 1000] } ] # best is C = 1
parameters = [ {'C': [0.001, 0.01, 1, 1.01, 1.1] } ] # best is still C = 1

# For Random Forest
parameters = [ {'n_estimators': [5, 10, 20, 100], 
                'criterion' : ['entropy', 'gini'],
                'max_features' : [100, 1000, 3000, 5000, 73561] }]
    # best are : 'n_estimators': 100, 'criterion' : gini', 'max_features' : 5000

### Non-Linear
    
# For Non-linear SVM    
parameters = [ {'C': [0.01, 0.1, 1, 10, 100], 
                'kernel' : ['linear', 'rbf', 'sigmoid', 'poly'], 
                'gamma' : [100, 10, 1, 0.1, 0.01, 0.001], 
                'degree' : [2, 3, 4, 5] } ]
    # best are : 'C': 0.1, 'degree': 2, 'gamma': 1, 'kernel': 'rbf'



grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, 
                           scoring = 'accuracy', refit = True, verbose = 2,
                           return_train_score=True, cv = 3 ) 

grid_search = grid_search.fit(X_train, y_train)
y_pred = grid_search.predict(X_test)     


best_accuracy = grid_search.best_score_
best_parameters = grid_search.best_params_
results = grid_search.cv_results_

print('Grid search best accuracy is: ', best_accuracy)
print('Grid search best parameters are: ', best_parameters)

cm = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
classification_Report = classification_report(y_test, y_pred, target_names 
                                              = ['Neg. Virus', 'Pos. Virus'], 
                                              digits =3)
print(classification_Report)
print('Accuracy is: ', accuracy)
print('Confusion Matrix is: ')
print(cm)

## END: Applying Grid Search to find the best model and the best parameters ##





#########################       END OF SCRIPT      ############################






